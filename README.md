#  Detecting and Reducing the Factual Hallucinations of Large Language Models with Metamorphic Testing

Question answering (QA) is a fundamental task of a large language model (LLM), which requires LLM to automatically answer human-posed questions in natural language. However, LLMs are known to distort facts and make non-factual statements (hallucination) when dealing with QA tasks, which may affect the deployment of LLMs in real-life situations. In this work, we present DrHall, a method for the detection of factual errors in black-box large language models inspired by metamorphosis testing in software testing. We believe that the model's hallucination answer is unstable. It is easier to produce different answers to the hallucination by using metamorphic relation (MR) to make the model take different execution paths for re-execution. We empirically evaluate DrHall on three datasets covering natural and code language data, finding that it outperforms existing methods and baselines, often by a large gap. In addition, by transforming DrHall using diverse path sampling, we obtain error correction methods with higher success rates. Our results demonstrate the potential of using MR to mitigate LLM hallucination.

## Table of Contents

- [RQ1](#RQ1)
- [RQ2](#RQ2)
- [RQ3](#RQ3)

## RQ1

### FactHalluQA Dataset Overview

- **Purpose**: FactHalluQA is designed to assess the hallucination in the Large Language Model (LLM) while mitigating the potential influence of benchmark leakage on the assessment process.
- **Composition**: The dataset consists of more than eight hundred questions in English, divided on the basis of disciplines, covering questions on physics, chemistry, biology, geography, history, literature, and so on.
- **Path**: The dataset is stored in this repository in the ```./RQ1/FactHalluQA``` path.


### **Data Examples**
We show some data examples of HalluQA here.
![](img/examples.png)

---


## RQ2
We present DrHall, a method for the detection of factual errors in black-box large language models inspired by metamorphic testing in software testing.
### Requirements

* PyQt6~=6.5.2
* PyQt6-sip~=13.5.2
* loguru~=0.7.0
* openpyxl~=3.1.2
* EdgeGPT~=0.13.2
* Wikipedia-API~=0.6.0
* pandas~=2.0.3
* openai~=0.28.0
* translatepy~=2.3
* pydantic~=2.0.3
* pydantic_core~=2.3.0

### Folder Structure

* `data`: This folder contains the data to be processed.
  * `data/test_example`: This folder contains more example data for testing in the thesis.
* `entity`: This folder contains the entity files for HTTP response contents from Bard, ChatGPT, NewBing and so on.
* `keys`: This folder contains the API keys for Bard, ChatGPT, NewBing and so on.
* `log`: This folder contains the log files generated during the execution.
* `tasks`: This folder contains the tasks that need to be completed. Files in this folder are the core algorithms
  concerning the thesis target.
* `ui`: This folder contains the UI files used by PyQt6, straightforwardly verifying the correctness of the answers
  generated by AI.
* `util`: This folder contains the utility files used by the program, most of the files with which are used for
  simplifying the code.
* `config.py`: This file contains the configuration parameters used by the program.
* `main.py`: This file contains the entry that runs the program.
* `main_tr.py`: This file contains the entry that translate the questions.


### Usage

1. Create a folder named `keys` and create a file named `chatgpt_api_key` within it. Enter a valid ChatGPT API Key in
   the format `sk-....`.
2. Rename the data file to `data.xlsx` and place it in the `data` folder.
3. **_Do not use only numbers to name each sheet in `data.xlsx`_**
4. Install the required packages by running `pip install -r requirements.txt`.
5. Run the script with `python main.py [-h] [-l LOG] [-m MODE]`.
6. Follow the prompts to enter the name or index (starting from 1) of the sheet you want to process.
7. Processing will begin.
8. Log files will be stored in the `log` folder.


## RQ3
By transforming DrHall using diverse path sampling, we obtain error correction methods with higher success rates. Our results demonstrate the potential of using MR to mitigate LLM hallucination.

### Requirements


### Folder Structure


### Usage


